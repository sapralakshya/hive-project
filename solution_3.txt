-> The reducer will work when we use limit 1 in any hive query.

-> If multiple clients trying to access hive at the same time then it will work completely fine  whatever tables are being created by different different clients their metadata will be         stored in hive metastore and the table is stored in the hive warehouse but the condition is our hadoop cluster should be big (have a suitable amout of nodes) so that if multiple clients perform any operation at same time it will be managed.

-> To optimize the processing time for hive query the solution is to create partitions:
    1.) create a statically partitioned table based on month field lets say table_part
    2.) static partitioning is done because we know that total 12 months data will be there and load the data accordingly also static partioning is faster than dynamic
    3.) insert the data in the table by writing command ' insert overwrite table table_part partition (month='jan') select * from transaction_details where month='jan';
    4.) similarly the data will be stored for all the months 
    5.) when you fire the hive query as ' select month, sum(amount) as revenue from table_part group by month;' this will give result faster.

-> In the above partitioned table creating a new partition for month dec will simply be done by :
    insert overwrite table table_part partition(month='dec') select * from transaction_details where month='dec';

-> for removing the semantic error or strict coloumn we just need to set one property before creating dynamically partitioned table i.e. set hive.exec.dynamic.partition.mode=nonstrict;

-> creating a csv table using built in serde :

   create table data_csv (
    id int,
    first_name string,
    last_name string,
    email string,
    gender string,
    ip_address string
   )
   row format serde 'org.apache.hadoop.hive.serde2.opencsvserde'
   with serdeproperties (
    "seperator" =","
     )
    stored as textfile;

-> The solution is we need to have all the csv files under one directory in hdfs and create the external table that will point to the directory and not to any individual file so the table will be pointig to all the files and whenever any query we run it will give the result accordingly.

-> The following command failed because whenever we try to load data from local system we need to specify 'file://' this file word as it gives indication to hive engine that file is to be loaded from loacl system.

-> Yes we can add any no of nodes ,it totally depends on our requirement adding new node to a cluster can be done by:
   . Install the application on the node depending on which role that node is to be given (like master node,or slave node or any other )
   . Connect the nodes to the server and create id password for that node 
   . In this way new nodes can be added to the cluster

practical questions

-> vim join_1.txt
   vim join_2.txt

   create table customers (
   cust_id int,
   name string,
   age int,
   address string,
   salary float
)
row format delimited
fields terminated by ',';

load data local inpath 'file:///home/cloudera/join_1.txt' into table customers;

create table order (
o_id int,
date string,
cust_id string,
amount float
)
row format delimited
fields terminated by ',';

load data local inpath 'file:///home/cloudera/join_2.txt' into table order;

inner join : select c.name,c.address,o.date.o.amount from customers c join order o on c.cust_id=o.cust_id;
left join : select c.name,c.address,o.date.o.amount from customers c left join order o on c.cust_id=o.cust_id;
right join : select c.name,c.address,o.date.o.amount from customers c right join order o on c.cust_id=o.cust_id;
full join : select c.name,c.address,o.date.o.amount from customers c full outer join order o on c.cust_id=o.cust_id;

-> create table air-quality (
   date string,
   time string,
   co float,
   pt08s1 float,
   nmhc int,
   c6h6 float,
   pt08s2 float,
   nox float,
   pt08s3 float,
   no2 float,
   pt08s4 float,
   pt08s5 float,
   t float,
   rh float,
   ah float
)
row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
with seredeproperties (
"separatorChar"= "\;",
"escapeChar" = "\\"
)
stored as textfile
tblproperties("skip.header.line.count"="1");

load data local inpath 'file:///home/cloudera/Air.csv' into table air_quality;

select * from air_quality limit 10;

saving select query in text file:
- in linux terminal type command hive -e "select * from sample_db.air_quality limit 10" > /home/cloudera/result.txt
- then check the file content using cat result.txt

select date,avg(co) as average_co from air_quality group by date;

filter examples: select date,co,t from air_quality where t>10;
                 select * from air_quality where date in ('10-03-2004','11-03-2004');
                 select * from air_quality where date like '10%';
                 select * from air_quality where pt08s1 between 1000 and 2000 limit 10;
                 select * from air_quality where t<rh limit 10;

regex example : select regexp_replace(date,"\-","\\") as date from air_quality limit 10;

alter table: alter table air_quality change nmhc nmhc float;

order by operation : select * from air_quality order by nmhc limit 10;

where clause :   select date,co,t from air_quality where t>10;

sorting : select * from air_quality sort by nmhc limit 10;

distinct : select count (distinct date) from air_quality;

like: select * from air_quality where date like '10%';

union : select date,co from air_quality where date like '10%'
        union all
        select date,co from air_quality where date like '11%';

view: create view air_qual_view as select * from air_quality limit 10;

drop: drop table air_quality;          